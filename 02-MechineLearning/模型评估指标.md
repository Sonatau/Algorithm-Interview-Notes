## 分类指标

以下指标用于二分类问题，如果是多分类的问题，也可以使用1:M的方法转变成二分类进行计算。

### 混淆矩阵 Confusion Matrix

<img src="https://tva1.sinaimg.cn/large/008i3skNly1gwyjq0tj34j30xi0dsmy5.jpg" height="200px" />

### 准确率 - accuracy

全局预测正确的样本数占所有样本数的比例，计算公式如下：
$$
Acc = \frac{TP + TN}{ALL}
$$
缺点：在binary classification且正反例不平衡的情况下，计算准确率没有意义。

假设在测试集里，有100个sample，99个反例，只有1个正例。如果模型对任意一个sample都预测为反例，Acc是 正确的个数／总个数 = 99/100 = 99%，但显然这样的预测是不合理的。

### 查准率 Precision

所有预测为正样本的集合中预测正确的比例：
$$
Precision = \frac{TP}{TP + FP}
$$

### 查全率 Recall

所有正样本中预测正确的比例，即正样本的准确率。
$$
Recall = \frac{TP}{TP + FN}
$$
如果你的模型很贪婪，想要覆盖更多的sample，那么它就更有可能犯错。在这种情况下，你会有很高的recall，但是较低的precision。

如果你的模型很保守，只对它很sure的sample作出预测，那么你的precision会很高，但是recall会相对低。

### F1值

综合精确率和召回率指标，相当于调和均值。计算公式如下：
$$
F1 = \frac{2 * Precision * Recall }{Precision + Recall}
$$
缺点：如果两个模型，一个precision特别高，recall特别低，另一个recall特别高，precision特别低的时候，f1-score可能是差不多的，因此也不能基于此来作出选择。

当类别不平衡时，可以使用PR曲线和ROC曲线进行模型评估。

### ROC曲线
以TPR(True Positive Rate，真正率，等于召回率)为纵轴：所有正例中被预测为正例的概率。
$$
TPR = \frac{TP}{P} = \frac{TP}{TP + FN}
$$

FPR(False Positive Rate，假正率，不是真正的正)为横轴：所有负例中被预测为正例的概率。
$$
FPR = \frac{FP}{N} =\frac{FP}{FP + TN}
$$

当一个样本被分类器判为正例，若其本身是正例，则TPR增加；

若其本身是负例，则FPR增加，因此ROC曲线可以看作是随着阈值的不断移动，所有样本中正例与负例之间的“对抗”。

曲线越靠近左上角，意味着越多的正例优先于负例，模型的整体表现也就越好。

#### 作图

在不同的分类阈值 (threshold) 设定下分别以TPR和FPR为纵、横轴作图。

1. 假设有P个正例，N个反例。
2. 分类器对所有样本的正负例进行预测，将得到一个probability score，**对所有样本按预测概率排序**。
3. 设定一个threshold，大于该值为正例，小于为负例。
4. 将分类阈值设为最大，即把所有样本均预测为反例，对应图上的点为 (0,0)。
5. 将分类阈值依次设为每个样本的预测概率，即依次将每个样本划分为正例，可以画出坐标轴中新的点
6. 所有样本点的TPR和FPR值，用线段相连，如下图黄线所示。



<img src="https://pic3.zhimg.com/80/v2-70f8ba09d21ca845a1ec0c107e41212e_1440w.jpg?source=1940ef5c" height="400px" />

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。

- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值（比如0.5）的话，能有预测价值。

- AUC = 0.5，跟随机猜测一样（例如：抛硬币猜正反面），模型没有预测价值。

- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

#### AUC (Area Under the Curve)

AUC需要计算折线下方的面积。

从所有正例中随机选取一个样本A，再从所有负例中随机选取一个样本B，分类器将A判为正例的概率比将B判为正例的概率大的可能性。

可以看到位于蓝色虚线上方的点(如图中的A点)被认为好于随机猜测。

在这样的点上TPR总大于FPR，意为正例被判为正例的概率大于负例被判为正例的概率。

<img src="https://pic3.zhimg.com/80/v2-2dc9d2148aefd4bf6ff879b3b85837e6_1440w.jpg" alt="img" height="300px" />

#### 优势

1. 注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，所以即使P或N的整体数量发生了改变，也不会影响到另一列。也就是说，**即使正负例变得不平衡，ROC曲线也不会产生大的变化**，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。

2. 在类别不平衡的背景下，**负例的数目众多致使FPR的增长不明显，导致ROC曲线呈现一个过分乐观的效果估计**。当负例N的数量远超正例P时，FP的大幅增长只能换来FPR的微小改变。结果是虽然大量负例被错判成正例，在ROC曲线上却无法直观地看出来。

    举个例子，假设一个数据集有正例20，负例10000，开始时有20个负例被错判，FPR=0.002 ，接着又有20个负例错判， FPR=0.004 ，在ROC曲线上这个变化是很细微的。而与此同时Precision则从原来的0.5下降到了0.33，在PR曲线上将会是一个大幅下降。

### PR曲线

以查全率为横轴，查准率为纵轴，所以称为P(Precision 纵)R(Recall 横)曲线。

<img src="https://pic1.zhimg.com/80/v2-6f09575f1a32692274177e7d2da55066_1440w.jpg?source=1940ef5c" height="250px" />

曲线下的面积(AUC)越大，或者说曲线更接近右上角（precision=1， recall=1），那么模型就越理想，越好。

**类别不平衡问题中，PR曲线则因为Precision的存在会不断显现FP的影响。**

### 适用场景

地震的时候，模型应该将每一次地震都预测到，查全率需要尽可能高，做出错误预测也没有关系。

人脸识别的时候，模型应该尽可能提高查准率，不然就会导致把陌生人放进去。

**ROC与PR的选择**

1. 如果想要评估在**相同**的类别分布下正例的预测情况，则宜选PR曲线。
2. 类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。
3. ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。

### 对数损失

在正负例不平衡的情况下，对数损失没有意义。

![img](https://upload-images.jianshu.io/upload_images/3007211-b8cb1dd10c07eb9c.png?imageMogr2/auto-orient/strip|imageView2/2/w/498)

## 回归指标

模型训练的时候，通过比较这些指标

### 均方误差 - Mean Squared Error

观测值与真值偏差的平方和与观测次数的比值：

$$
MSE = \frac{1}{m}\sum_{i=1}^{m}(f_i-y_i)^2
$$

### 均方根误差 - Root Mean Squared Error

它的意义在于开个根号后，误差的结果就与数据是一个级别的，可以更好地来描述数据。标准误差对一组测量中的特大或特小误差反映非常敏感

$$
RMSE =\sqrt{\frac{1}{m}\sum_{i=1}^{m}(f_i-y_i)^2}
$$

### 平均绝对误差 - Mean Absolute Error

平均绝对误差是绝对误差的平均值，能更好地反映预测值误差的实际情况。

$$
MAE = \frac{1}{m}\sum_{i=1}^{m}|f_i-y_i|
$$

## 参考博客

- https://zhuanlan.zhihu.com/p/34655990
- https://www.jianshu.com/p/ce70c716c9d1
