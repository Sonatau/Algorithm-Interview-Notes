### 预防过拟合的方式

1. 降低模型复杂度
2. 调整模型停止训练的时间
3. 数据增强：增加数据量
4. 数据清洗：可以去除一些噪声干扰
5. 正则化：控制参数的大小，避免函数值在很小的区间内变化剧烈，进而避免过拟合
6. **dropout**：训练的时候随机丢弃一些神经元不参与训练，可以避免训练过程中一些位置敏感或者依赖性（避免某个神经元作用比较大）其实也就是防止参数过分依赖训练数据，增加参数对数据集的泛化能力。
7. BN也可以一定程度上避免过拟合：BN的使用使得一个mini-batch中的所有样本都被关联在了一起，因此网络不会从某一个训练样本中生成确定的结果，相当于一个间接的数据增强，达到防止**过拟合**作用.

### 用正则化项防止过拟合的原理

**正则化函数通常是关于模型待求权重向量`w`的函数，随着模型的复杂度而单调递增。因此通过约束结构损失以限制模型参数，在一定程度上减少过拟合情况。**

过拟合意味着拟合函数需要顾忌每一个点，最终形成的拟合函数波动很大。在某些很小的区间里，函数值的变化很剧烈。这就意味着函数在某些小区间里的导数值（绝对值）非常大。由于自变量值可大可小，所以只有系数足够大，才能保证导数值很大。

![](https://tva1.sinaimg.cn/large/008i3skNly1gw9w0n7v1ej30670600sm.jpg)

### 权重向量`w`稀疏的作用

为了**实现特征的自动选择**。

1. 对于数据集中的某一样本，并不是每个特征都是需要被利用的；有些特征甚至属于干扰项。
2. 因此稀疏矩阵`w`可以学习如何去掉没有信息的特征，也就是将相应的$w_j$置为0。

### Dropout的作用

防止参数过分依赖训练数据，减少神经元之间复杂的共适应关系，增加参数对数据集的泛化能力。
